{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/ipsa_logo.png\" width=\"100\" align=\"right\">\n",
    "\n",
    "# Ma422- 2022/2023\n",
    "\n",
    "# <font color='black'> Introduction to Machine Learning</font>\n",
    "\n",
    "---\n",
    "1. [Set-Up](#Set-Up)\n",
    "2. [Linear regression with one variable](#LinearRegression)\n",
    " * [Visualize the Word count vs. #Shares](#VS)\n",
    " * [Gradient Descent](#GD)\n",
    "3. [Linear regression with multiple variables](#LinearRegression2)\n",
    " * [Feature Scaling](#FeatureScaling)\n",
    " * [Gradient Descent](#GD2)\n",
    " * [Normal Equation](#NE)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Intoduction to Machine Learning Course.\n",
    "\n",
    "The objective of this lab session is to code linear regression and to apply the algorithm to real data. You will predict how many times a Machine Learning article will be shared on Social Networks according to some of its characteristics.\n",
    "\n",
    "\n",
    "## <font color='black'>Set-Up</font>\n",
    "\n",
    "---\n",
    "\n",
    "First we will import the packages that we will need throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Jupyter to display plots in this notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the numpy package with the alias np.\n",
    "import numpy as np           \n",
    "import pandas as pd\n",
    "\n",
    "# Import the pyplot package from matplotlib with the alias plt.\n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pylab\n",
    "pylab.rcParams['figure.figsize'] = (4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='black'>Linear regression with one variable</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on any task, it is often useful to understand the data by visualizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Load the data =======================\n",
    "data = data = pd.read_csv(\"./data/articulos_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Check the dimensions ================\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Let's visualize the first rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark that some entries (as for instance in the comments column) are empty.\n",
    "\n",
    "In our case, the column Shares will correspond to the target value, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Display some statistics from the dataset =======================\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, we can read that the the mean number of words is 1808.\n",
    "There is an article with a minimum of 250 words and a maximum of 8401.\n",
    "\n",
    "Concerning the outputs, the minimum is 0 times shared and the maximum is shared 350000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Word count vs. #Shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = data['Word count'].values\n",
    "f2 = data['# Shares'].values\n",
    "    \n",
    "plt.scatter(f1, f2, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply a cut to the oringal dataset\n",
    "filtered_data = data[(data['Word count'] <= 3500) & (data['# Shares'] <= 80000)]\n",
    "\n",
    "f1 = filtered_data['Word count'].values\n",
    "f2 = filtered_data['# Shares'].values\n",
    "    \n",
    "plt.scatter(f1, f2, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX =filtered_data[[\"Word count\"]]\n",
    "X = np.array(dataX)\n",
    "y = filtered_data['# Shares'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'> Gradient Descent</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will fit the linear regression parameters  $\\theta$  to the dataset using gradient descent.\n",
    "\n",
    "Recall that the parameters of your model are the  $\\theta_ùëó$  values. These are the values you will adjust to minimize cost  $ùêΩ(\\theta)$ .\n",
    "\n",
    "In the following lines, we add another dimension to our data to account for the  $\\theta_0$  intercept term. We also initialize the parameters to $0$ and the learning rate  $\\alpha$  to $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Cost and Gradient descent ===================\n",
    "\n",
    "# Setup the data matrix appropriately, and add ones for the intercept term\n",
    "m, n = X.shape\n",
    "\n",
    "# Add intercept term to X\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "theta = np.zeros(2); # initialize fitting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you perform gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. \n",
    "\n",
    "Firstly you will implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent.\n",
    "\n",
    "Complete the function below that computes $J(\\theta)$. \n",
    "Once you have completed the function, the next step will run computeCost once using $\\theta$ initialized to zeros, and you will see the cost printed to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "def computeCost(X, y, theta):\n",
    "    #COMPUTECOST Compute cost for linear regression\n",
    "    #   using theta as the parameter for linear regression to fit the data points in X and y\n",
    "    \n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "    #You need to return the following variables correctly \n",
    "    J = 0;\n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta);\n",
    "\n",
    "print('With theta = [0 ; 0] Cost computed = ' , J);\n",
    "print('Expected cost value (approx) 408398897.763\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement gradient descent. The loop structure has already been written. You only need to supply the updates to $\\theta$ within each iteration.\n",
    "\n",
    "A good way to verify that gradient descent is working is to look at the value of $J(\\theta)$ and check that it is decreasing with each step. The code calls computeCost on every iteration and prints the cost. If you have implemented gradient descent and computeCost correctly, your value of $J(\\theta)$ should never increase, and should converge to a  value by the end of the algorithm.\n",
    "\n",
    "After you have finished, you will use the final parameters to plot the linear fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
    "    # are passed by reference to functions\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # ====================================================================\n",
    "         \n",
    "\n",
    "            \n",
    "            \n",
    "        # =====================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "    \n",
    "    return theta, J_history   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(2); # initialize fitting parameters\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1000;\n",
    "alpha = 1e-8;\n",
    "\n",
    "theta, J = gradientDescent(X, y, theta, alpha, iterations);\n",
    "\n",
    "print('Theta found by gradient descent:\\n');\n",
    "print(theta);\n",
    "print('Expected theta values (approx)\\n');\n",
    "print(' 0.0278\\n  11.162\\n\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='black'> Linear regression with multiple variables</font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will implement linear regression with multiple variables to predict the number of shares of a machine learning article. We will create a new variable that is the sum of the links, comments and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Load the data =======================\n",
    "\n",
    "suma = (filtered_data[\"# of Links\"] + filtered_data['# of comments'].fillna(0) + filtered_data['# Images video'])\n",
    "\n",
    "dataX2 =  pd.DataFrame()\n",
    "dataX2[\"Word count\"] = filtered_data[\"Word count\"]\n",
    "dataX2[\"suma\"] = suma\n",
    "X = np.array(dataX2)\n",
    "y = filtered_data['# Shares'].values\n",
    "\n",
    "m = y.shape[0] # number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "By looking at the values, note that the word count are about 1000 times the sum of the other variables. When features differ by orders of magnitude, performing feature scaling can make gradient descent converge much more quickly.\n",
    "\n",
    "You need to fill the cell below to perform feature scaling.\n",
    "\n",
    "Subtract the mean value of each feature from the dataset.\n",
    "After subtracting the mean, additionally scale (divide) the feature values by their respective ‚Äústandard deviation\".\n",
    "When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation.\n",
    "\n",
    "Given a new input x, we must first normalize x using the mean and standard deviation that we had previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "def featureNormalize(X):\n",
    "    #FEATURENORMALIZE Normalizes the features in X \n",
    "    #   FEATURENORMALIZE(X) returns a normalized version of X where\n",
    "    #   the mean value of each feature is 0 and the standard deviation\n",
    "    #   is 1. This is often a good preprocessing step to do when\n",
    "    #   working with learning algorithms.\n",
    "\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    # ===========================================================\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, mu, sigma = featureNormalize(X);\n",
    "\n",
    "#Add intercept term to X\n",
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'> Gradient Descent</font>\n",
    "\n",
    "\n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in the matrix X. \n",
    "\n",
    "The hypothesis function and the batch gradient descent update rule remain unchanged.\n",
    "If your code in the previous part (single variable) already supports multiple variables, you can use it here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "def computeCostMulti(X, y, theta):\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0\n",
    "    \n",
    "    # ===============================================================\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ==================================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "    #GRADIENTDESCENTMULTI Performs gradient descent to learn theta\n",
    "    #   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by\n",
    "    #   taking num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    # make a copy of theta, which will be updated by gradient descent\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = []\n",
    "        \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # =============================================================\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ==============================================================\n",
    "\n",
    "        # Save the cost J in every iteration    \n",
    "        J_history.append(computeCostMulti(X, y, theta))\n",
    "\n",
    "                                      \n",
    "    return theta, J_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Gradient Descent ================\n",
    "\n",
    "# Choose some alpha value\n",
    "alpha = 0.1;\n",
    "num_iters = 50;\n",
    "\n",
    "# Init Theta and Run Gradient Descent \n",
    "theta = np.zeros(3);\n",
    "\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will to try out different learning rates for the dataset and find a learning rate that converges quickly. You can change the learning rate by modifying the variable above and changing the part of the code that sets the learning rate.\n",
    "\n",
    "The cell bellow plots the values of  $ùêΩ(\\theta)$  against the number of the iterations. If you picked a good value for the learning rate, the cost will decrease in every iteration. If the value of $J(\\theta)$ increases, adjust your learning rate and try again. You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0,num_iters), J_history, '-b');\n",
    "plt.xlabel('Number of iterations');\n",
    "plt.ylabel('Cost J');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display gradient descent's result\n",
    "print('Theta computed from gradient descent: \\n');\n",
    "print(theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the value of a a new input x = [515, 7]\n",
    "\n",
    "# ===========================================================\n",
    "# Recall you need to add the column of ones and normalize the input vector\n",
    "\n",
    "shares = None\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Predicted number of shares: \\n', shares)\n",
    "print('True number of shares: 27000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "\n",
    "We have seen that the closed-form solution to linear regression is\n",
    "\n",
    "$$ \\theta = \\left( X^T X\\right)^{-1} X^T {y}$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \"loop until convergence‚Äù like in gradient descent.\n",
    "Complete the code below to use the formula to calculate $\\theta$. Remember that while you don‚Äôt need to scale your features, we still need to add a column of 1‚Äôs to the X matrix to have an intercept term ($\\theta_0$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Normal Equations ================\n",
    "\n",
    "# Load Data\n",
    "suma = (filtered_data[\"# of Links\"] + filtered_data['# of comments'].fillna(0) + filtered_data['# Images video'])\n",
    "\n",
    "dataX2 =  pd.DataFrame()\n",
    "dataX2[\"Word count\"] = filtered_data[\"Word count\"]\n",
    "dataX2[\"suma\"] = suma\n",
    "X = np.array(dataX2)\n",
    "y = filtered_data['# Shares'].values\n",
    "\n",
    "m = y.shape[0] # number of training examples\n",
    "\n",
    "# Add intercept term to X\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the cell below for the function `normalEqn`. You should use the formula above to calculate $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "def normalEqn(X, y):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # ================================================================\n",
    "\n",
    "    \n",
    "    # =================================================================\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the parameters from the normal equation\n",
    "theta = normalEqn(X, y);\n",
    "\n",
    "#Display normal equation's result\n",
    "print('Theta computed from the normal equation : \\n');\n",
    "print(theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the value of a a new input x = [515, 7]\n",
    "\n",
    "# ============================================================\n",
    "# Recall you need to add the column of ones and normalize the input vector\n",
    "\n",
    "shares = None\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Predicted number of shares: (using normal equations)\\n', shares)\n",
    "print('True number of shares: 27000')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
